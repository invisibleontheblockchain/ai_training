#!/usr/bin/env python3
"""
Comprehensive Model Testing Framework
=====================================

This script provides a complete testing framework to evaluate your AI model's capabilities
across multiple dimensions including coding, reasoning, knowledge, autonomy, and performance.

Usage:
    python comprehensive_model_test_prompt.py [--model-path MODEL_PATH] [--test-level LEVEL]

Test Levels:
    - basic: Simple Q&A, basic coding, arithmetic
    - intermediate: Technical knowledge, algorithms, complex reasoning
    - advanced: System design, optimization, multi-step problems
    - expert: Research-level questions, novel problem solving
    - autonomous: Tests ability to work without clarifying questions

Created: June 4, 2025
"""

import json
import time
import logging
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'model_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestCase:
    """Individual test case"""
    id: str
    category: str
    difficulty: str
    task_type: str
    prompt: str
    expected_keywords: List[str]
    min_response_length: int
    max_response_time: float
    evaluation_criteria: Dict[str, Any]
    test_autonomy: bool = False
    context: Optional[str] = None

@dataclass
class TestResult:
    """Result of a single test"""
    test_id: str
    prompt: str
    response: str
    response_time: float
    response_length: int
    passed: bool
    quality_score: float
    autonomy_score: float
    keyword_matches: int
    errors: List[str]
    metadata: Dict[str, Any]

@dataclass
class TestSuiteResults:
    """Results from a complete test suite"""
    suite_name: str
    timestamp: str
    total_tests: int
    passed_tests: int
    average_quality: float
    average_autonomy: float
    average_response_time: float
    test_results: List[TestResult]
    summary: Dict[str, Any]

class ComprehensiveModelTester:
    """Main testing framework class"""
    
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = model_path
        self.test_suites = self._create_test_suites()
        self.results_dir = Path("test_results")
        self.results_dir.mkdir(exist_ok=True)
        
    def _create_test_suites(self) -> Dict[str, List[TestCase]]:
        """Create comprehensive test suites for different difficulty levels"""
        
        return {
            "basic": [
                TestCase(
                    id="basic_001",
                    category="conversation",
                    difficulty="basic",
                    task_type="general",
                    prompt="Hello! Can you introduce yourself and explain what you can help me with?",
                    expected_keywords=["hello", "help", "assist", "AI", "language model"],
                    min_response_length=50,
                    max_response_time=5.0,
                    evaluation_criteria={
                        "politeness": 0.3,
                        "clarity": 0.4,
                        "informativeness": 0.3
                    }
                ),
                
                TestCase(
                    id="basic_002",
                    category="arithmetic",
                    difficulty="basic",
                    task_type="reasoning",
                    prompt="Calculate the result of: 15 + 23 ร 4 - 8 รท 2. Show your work step by step.",
                    expected_keywords=["15", "23", "4", "8", "2", "order", "operations", "PEMDAS", "BODMAS"],
                    min_response_length=80,
                    max_response_time=10.0,
                    evaluation_criteria={
                        "correct_answer": 0.5,
                        "step_by_step": 0.3,
                        "explanation": 0.2
                    }
                ),
                
                TestCase(
                    id="basic_003",
                    category="simple_coding",
                    difficulty="basic",
                    task_type="coding",
                    prompt="Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.",
                    expected_keywords=["def", "function", "list", "sum", "even", "return", "%", "2"],
                    min_response_length=100,
                    max_response_time=15.0,
                    evaluation_criteria={
                        "correct_syntax": 0.3,
                        "logic_correctness": 0.4,
                        "code_style": 0.2,
                        "documentation": 0.1
                    }
                ),
                
                TestCase(
                    id="basic_004",
                    category="knowledge",
                    difficulty="basic",
                    task_type="knowledge",
                    prompt="Explain the difference between a variable and a constant in programming, with examples.",
                    expected_keywords=["variable", "constant", "change", "value", "example", "programming"],
                    min_response_length=120,
                    max_response_time=10.0,
                    evaluation_criteria={
                        "accuracy": 0.4,
                        "clarity": 0.3,
                        "examples": 0.3
                    }
                )
            ],
            
            "intermediate": [
                TestCase(
                    id="inter_001",
                    category="algorithm_implementation",
                    difficulty="intermediate",
                    task_type="coding",
                    prompt="Implement a binary search algorithm in Python. Include error handling for edge cases and provide time complexity analysis.",
                    expected_keywords=["binary", "search", "algorithm", "left", "right", "mid", "sorted", "O(log n)", "complexity"],
                    min_response_length=200,
                    max_response_time=30.0,
                    evaluation_criteria={
                        "algorithm_correctness": 0.4,
                        "error_handling": 0.2,
                        "complexity_analysis": 0.2,
                        "code_quality": 0.2
                    }
                ),
                
                TestCase(
                    id="inter_002",
                    category="data_structures",
                    difficulty="intermediate",
                    task_type="coding",
                    prompt="Design and implement a thread-safe queue data structure in Python using appropriate synchronization primitives.",
                    expected_keywords=["queue", "thread", "safe", "lock", "threading", "synchronization", "FIFO", "deque"],
                    min_response_length=250,
                    max_response_time=45.0,
                    evaluation_criteria={
                        "thread_safety": 0.4,
                        "implementation_quality": 0.3,
                        "performance_considerations": 0.2,
                        "documentation": 0.1
                    }
                ),
                
                TestCase(
                    id="inter_003",
                    category="problem_solving",
                    difficulty="intermediate",
                    task_type="reasoning",
                    prompt="A company has 100 employees. 60% work in engineering, 30% in sales, and 10% in management. If the company grows by 50% and maintains the same proportions, but needs to add 15 more managers than the proportion suggests, how many people will be in each department?",
                    expected_keywords=["100", "employees", "60%", "30%", "10%", "50%", "150", "90", "45", "15", "engineering", "sales", "management"],
                    min_response_length=150,
                    max_response_time=20.0,
                    evaluation_criteria={
                        "correct_calculation": 0.5,
                        "clear_reasoning": 0.3,
                        "step_by_step": 0.2
                    }
                ),
                
                TestCase(
                    id="inter_004",
                    category="technical_knowledge",
                    difficulty="intermediate",
                    task_type="knowledge",
                    prompt="Explain the differences between REST and GraphQL APIs, including their advantages, disadvantages, and appropriate use cases.",
                    expected_keywords=["REST", "GraphQL", "API", "HTTP", "query", "mutation", "over-fetching", "under-fetching", "caching"],
                    min_response_length=300,
                    max_response_time=25.0,
                    evaluation_criteria={
                        "technical_accuracy": 0.4,
                        "comparison_quality": 0.3,
                        "use_cases": 0.2,
                        "depth": 0.1
                    }
                )
            ],
            
            "advanced": [
                TestCase(
                    id="adv_001",
                    category="system_design",
                    difficulty="advanced",
                    task_type="reasoning",
                    prompt="Design a scalable real-time chat application that can handle 1 million concurrent users. Include architecture decisions, technology choices, database design, caching strategies, and deployment considerations.",
                    expected_keywords=["scalable", "real-time", "websocket", "microservices", "database", "caching", "load balancer", "CDN", "sharding"],
                    min_response_length=500,
                    max_response_time=60.0,
                    evaluation_criteria={
                        "architectural_soundness": 0.3,
                        "scalability_considerations": 0.3,
                        "technology_choices": 0.2,
                        "practical_implementation": 0.2
                    }
                ),
                
                TestCase(
                    id="adv_002",
                    category="optimization",
                    difficulty="advanced",
                    task_type="coding",
                    prompt="You have a function that processes large datasets but is running slowly. Here's the function:\n\ndef process_data(data):\n    result = []\n    for item in data:\n        if item['value'] > 100:\n            processed = expensive_calculation(item)\n            if processed not in result:\n                result.append(processed)\n    return result\n\nOptimize this function for better performance and explain your optimizations.",
                    expected_keywords=["optimization", "performance", "set", "comprehension", "cache", "vectorization", "parallel", "algorithm"],
                    min_response_length=300,
                    max_response_time=45.0,
                    evaluation_criteria={
                        "optimization_techniques": 0.4,
                        "code_improvement": 0.3,
                        "explanation_quality": 0.2,
                        "performance_analysis": 0.1
                    }
                ),
                
                TestCase(
                    id="adv_003",
                    category="machine_learning",
                    difficulty="advanced",
                    task_type="knowledge",
                    prompt="Design a machine learning pipeline for a recommendation system that needs to handle both collaborative filtering and content-based filtering. Include data preprocessing, model architecture, training strategy, evaluation metrics, and production deployment considerations.",
                    expected_keywords=["collaborative filtering", "content-based", "recommendation", "pipeline", "preprocessing", "evaluation", "cold start", "matrix factorization"],
                    min_response_length=400,
                    max_response_time=50.0,
                    evaluation_criteria={
                        "ml_knowledge": 0.4,
                        "pipeline_design": 0.3,
                        "production_readiness": 0.2,
                        "evaluation_strategy": 0.1
                    }
                ),
                
                TestCase(
                    id="adv_004",
                    category="complex_reasoning",
                    difficulty="advanced",
                    task_type="reasoning",
                    prompt="A tech startup is deciding between three growth strategies: (A) Focus on product features to increase user engagement, (B) Expand to new markets internationally, or (C) Acquire smaller competitors. They have limited resources: $2M budget, 50 engineers, 12-month timeline. Analyze each strategy considering market conditions, resource constraints, risks, and potential ROI. Provide a recommendation with justification.",
                    expected_keywords=["strategy", "analysis", "resources", "ROI", "risk", "market", "budget", "timeline", "recommendation"],
                    min_response_length=400,
                    max_response_time=55.0,
                    evaluation_criteria={
                        "strategic_thinking": 0.3,
                        "analysis_depth": 0.3,
                        "practical_considerations": 0.2,
                        "recommendation_quality": 0.2
                    }
                )
            ],
            
            "expert": [
                TestCase(
                    id="exp_001",
                    category="research_problem",
                    difficulty="expert",
                    task_type="reasoning",
                    prompt="Propose a novel approach to reduce the computational complexity of training large language models while maintaining performance. Consider both algorithmic innovations and hardware optimizations. Include theoretical analysis, implementation strategy, and potential challenges.",
                    expected_keywords=["computational complexity", "language models", "algorithmic", "optimization", "theoretical", "implementation", "challenges"],
                    min_response_length=500,
                    max_response_time=75.0,
                    evaluation_criteria={
                        "novelty": 0.3,
                        "technical_depth": 0.3,
                        "feasibility": 0.2,
                        "theoretical_foundation": 0.2
                    }
                ),
                
                TestCase(
                    id="exp_002",
                    category="interdisciplinary",
                    difficulty="expert",
                    task_type="knowledge",
                    prompt="How might quantum computing principles be applied to solve the protein folding problem more efficiently than classical approaches? Discuss the theoretical foundations, current limitations, required quantum algorithms, and potential breakthrough implications for drug discovery.",
                    expected_keywords=["quantum computing", "protein folding", "quantum algorithms", "drug discovery", "theoretical", "limitations", "breakthrough"],
                    min_response_length=450,
                    max_response_time=70.0,
                    evaluation_criteria={
                        "interdisciplinary_knowledge": 0.4,
                        "scientific_accuracy": 0.3,
                        "innovation_potential": 0.2,
                        "practical_implications": 0.1
                    }
                )
            ],
            
            "autonomous": [
                TestCase(
                    id="auto_001",
                    category="autonomous_coding",
                    difficulty="intermediate",
                    task_type="coding",
                    prompt="Create a complete web application for task management without asking any clarifying questions. Make all necessary design decisions yourself.",
                    expected_keywords=["web application", "task management", "HTML", "CSS", "JavaScript", "backend", "database", "CRUD"],
                    min_response_length=400,
                    max_response_time=60.0,
                    evaluation_criteria={
                        "completeness": 0.4,
                        "autonomy": 0.3,
                        "design_decisions": 0.2,
                        "code_quality": 0.1
                    },
                    test_autonomy=True
                ),
                
                TestCase(
                    id="auto_002", 
                    category="autonomous_analysis",
                    difficulty="advanced",
                    task_type="reasoning",
                    prompt="Analyze the current state of remote work and provide actionable recommendations for companies. Don't ask for clarification - make reasonable assumptions and be comprehensive.",
                    expected_keywords=["remote work", "analysis", "recommendations", "companies", "productivity", "challenges", "solutions"],
                    min_response_length=350,
                    max_response_time=50.0,
                    evaluation_criteria={
                        "comprehensiveness": 0.3,
                        "autonomy": 0.3,
                        "actionability": 0.2,
                        "insight_quality": 0.2
                    },
                    test_autonomy=True
                ),
                
                TestCase(
                    id="auto_003",
                    category="autonomous_design",
                    difficulty="advanced", 
                    task_type="reasoning",
                    prompt="Design a learning curriculum for someone wanting to become a data scientist. Include timeline, resources, projects, and assessment methods.",
                    expected_keywords=["curriculum", "data scientist", "timeline", "resources", "projects", "assessment", "skills", "learning"],
                    min_response_length=400,
                    max_response_time=55.0,
                    evaluation_criteria={
                        "curriculum_structure": 0.3,
                        "autonomy": 0.3,
                        "practical_applicability": 0.2,
                        "comprehensiveness": 0.2
                    },
                    test_autonomy=True
                )
            ]
        }
    
    def evaluate_response(self, test_case: TestCase, response: str, response_time: float) -> TestResult:
        """Evaluate a model response against test criteria"""
        
        # Basic metrics
        response_length = len(response)
        passed_length = response_length >= test_case.min_response_length
        passed_time = response_time <= test_case.max_response_time
        
        # Keyword matching
        response_lower = response.lower()
        keyword_matches = sum(1 for keyword in test_case.expected_keywords 
                            if keyword.lower() in response_lower)
        keyword_score = keyword_matches / len(test_case.expected_keywords) if test_case.expected_keywords else 0
        
        # Quality assessment based on task type
        quality_score = self._assess_quality(test_case, response)
        
        # Autonomy assessment (for autonomous tests)
        autonomy_score = self._assess_autonomy(response) if test_case.test_autonomy else 1.0
        
        # Overall pass/fail
        passed = (passed_length and passed_time and 
                 keyword_score >= 0.3 and quality_score >= 0.5)
        
        errors = []
        if not passed_length:
            errors.append(f"Response too short: {response_length} < {test_case.min_response_length}")
        if not passed_time:
            errors.append(f"Response too slow: {response_time:.2f}s > {test_case.max_response_time}s")
        if keyword_score < 0.3:
            errors.append(f"Insufficient keyword matches: {keyword_score:.2f} < 0.3")
        if quality_score < 0.5:
            errors.append(f"Quality score too low: {quality_score:.2f} < 0.5")
            
        return TestResult(
            test_id=test_case.id,
            prompt=test_case.prompt,
            response=response,
            response_time=response_time,
            response_length=response_length,
            passed=passed,
            quality_score=quality_score,
            autonomy_score=autonomy_score,
            keyword_matches=keyword_matches,
            errors=errors,
            metadata={
                "category": test_case.category,
                "difficulty": test_case.difficulty,
                "task_type": test_case.task_type,
                "keyword_score": keyword_score
            }
        )
    
    def _assess_quality(self, test_case: TestCase, response: str) -> float:
        """Assess response quality based on task type"""
        score = 0.0
        
        if test_case.task_type == "coding":
            score = self._assess_code_quality(response)
        elif test_case.task_type == "reasoning":
            score = self._assess_reasoning_quality(response)
        elif test_case.task_type == "knowledge":
            score = self._assess_knowledge_quality(response)
        else:
            score = self._assess_general_quality(response)
            
        return min(score, 1.0)
    
    def _assess_code_quality(self, response: str) -> float:
        """Assess code quality"""
        score = 0.0
        response_lower = response.lower()
        
        # Check for code structure
        code_indicators = ['def ', 'class ', 'import ', 'return ', 'if ', 'for ', 'while ']
        if any(indicator in response_lower for indicator in code_indicators):
            score += 0.4
            
        # Check for proper formatting
        lines = response.split('\n')
        indented_lines = sum(1 for line in lines if line.startswith('    ') or line.startswith('\t'))
        if indented_lines > 0:
            score += 0.2
            
        # Check for comments/documentation
        if '#' in response or '"""' in response or "'''" in response:
            score += 0.2
            
        # Check for error handling
        if any(term in response_lower for term in ['try:', 'except:', 'error', 'exception']):
            score += 0.2
            
        return score
    
    def _assess_reasoning_quality(self, response: str) -> float:
        """Assess reasoning quality"""
        score = 0.0
        response_lower = response.lower()
        
        # Check for logical connectors
        logical_words = ['because', 'therefore', 'since', 'thus', 'hence', 'so', 'first', 'second', 'finally']
        if any(word in response_lower for word in logical_words):
            score += 0.3
            
        # Check for structured thinking
        if any(phrase in response_lower for phrase in ['step 1', 'step 2', 'first,', 'second,', 'next,']):
            score += 0.3
            
        # Check for analysis indicators
        if any(word in response_lower for word in ['analysis', 'consider', 'compare', 'evaluate', 'assess']):
            score += 0.2
            
        # Check for numerical calculations or evidence
        if any(char.isdigit() for char in response):
            score += 0.2
            
        return score
    
    def _assess_knowledge_quality(self, response: str) -> float:
        """Assess knowledge quality"""
        score = 0.0
        
        # Check response depth
        if len(response) > 200:
            score += 0.3
        elif len(response) > 100:
            score += 0.2
            
        # Check for examples
        if any(phrase in response.lower() for phrase in ['example', 'for instance', 'such as', 'e.g.']):
            score += 0.3
            
        # Check for structured information
        sentences = response.split('.')
        if len(sentences) > 3:
            score += 0.2
            
        # Check for technical terminology (indicates domain knowledge)
        response_lower = response.lower()
        technical_indicators = ['algorithm', 'system', 'process', 'method', 'technique', 'approach']
        if any(term in response_lower for term in technical_indicators):
            score += 0.2
            
        return score
    
    def _assess_general_quality(self, response: str) -> float:
        """Assess general response quality"""
        score = 0.0
        
        # Basic coherence check
        if len(response) > 20:
            score += 0.3
            
        # Check for complete sentences
        if '.' in response:
            score += 0.2
            
        # Check for politeness/helpfulness
        if any(word in response.lower() for word in ['help', 'assist', 'glad', 'happy', 'pleased']):
            score += 0.2
            
        # Check for structure
        if any(char in response for char in ['\n', ':', '-', 'โข']):
            score += 0.3
            
        return score
    
    def _assess_autonomy(self, response: str) -> float:
        """Assess autonomy (fewer questions = higher autonomy)"""
        question_count = response.count('?')
        
        # Check for clarification-seeking phrases
        clarification_phrases = [
            'do you want', 'would you like', 'could you clarify', 'please specify',
            'what kind of', 'which type', 'more details', 'can you provide',
            'need to know', 'could you tell me', 'what should'
        ]
        
        clarification_count = sum(1 for phrase in clarification_phrases 
                                if phrase in response.lower())
        
        total_issues = question_count + clarification_count
        
        if total_issues == 0:
            return 1.0
        elif total_issues <= 1:
            return 0.8
        elif total_issues <= 2:
            return 0.6
        elif total_issues <= 3:
            return 0.4
        else:
            return 0.2
    
    def run_test_suite(self, suite_name: str, model_function: callable) -> TestSuiteResults:
        """Run a complete test suite"""
        logger.info(f"Running test suite: {suite_name}")
        
        if suite_name not in self.test_suites:
            raise ValueError(f"Unknown test suite: {suite_name}")
            
        test_cases = self.test_suites[suite_name]
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"Running test {i}/{len(test_cases)}: {test_case.id}")
            logger.info(f"Prompt: {test_case.prompt[:100]}...")
            
            try:
                start_time = time.time()
                response = model_function(test_case.prompt)
                response_time = time.time() - start_time
                
                result = self.evaluate_response(test_case, response, response_time)
                results.append(result)
                
                logger.info(f"Result: {'PASS' if result.passed else 'FAIL'} "
                          f"(Quality: {result.quality_score:.2f}, "
                          f"Time: {result.response_time:.2f}s)")
                
            except Exception as e:
                logger.error(f"Error running test {test_case.id}: {str(e)}")
                results.append(TestResult(
                    test_id=test_case.id,
                    prompt=test_case.prompt,
                    response=f"ERROR: {str(e)}",
                    response_time=0.0,
                    response_length=0,
                    passed=False,
                    quality_score=0.0,
                    autonomy_score=0.0,
                    keyword_matches=0,
                    errors=[str(e)],
                    metadata={"category": test_case.category, "error": True}
                ))
        
        # Calculate summary statistics
        passed_tests = sum(1 for r in results if r.passed)
        avg_quality = sum(r.quality_score for r in results) / len(results) if results else 0
        avg_autonomy = sum(r.autonomy_score for r in results) / len(results) if results else 0
        avg_time = sum(r.response_time for r in results) / len(results) if results else 0
        
        summary = {
            "pass_rate": passed_tests / len(results) if results else 0,
            "average_quality": avg_quality,
            "average_autonomy": avg_autonomy,
            "average_response_time": avg_time,
            "total_tests": len(results),
            "passed_tests": passed_tests,
            "failed_tests": len(results) - passed_tests,
            "test_categories": list(set(r.metadata.get("category", "unknown") for r in results))
        }
        
        return TestSuiteResults(
            suite_name=suite_name,
            timestamp=datetime.now().isoformat(),
            total_tests=len(results),
            passed_tests=passed_tests,
            average_quality=avg_quality,
            average_autonomy=avg_autonomy,
            average_response_time=avg_time,
            test_results=results,
            summary=summary
        )
    
    def run_all_tests(self, model_function: callable, test_levels: List[str] = None) -> Dict[str, TestSuiteResults]:
        """Run all test suites"""
        if test_levels is None:
            test_levels = ["basic", "intermediate", "advanced", "autonomous"]
            
        all_results = {}
        
        for level in test_levels:
            if level in self.test_suites:
                results = self.run_test_suite(level, model_function)
                all_results[level] = results
            else:
                logger.warning(f"Unknown test level: {level}")
        
        return all_results
    
    def save_results(self, results: Dict[str, TestSuiteResults], filename: str = None):
        """Save test results to JSON file"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"model_test_results_{timestamp}.json"
            
        filepath = self.results_dir / filename
        
        # Convert dataclass objects to dictionaries for JSON serialization
        json_results = {}
        for suite_name, suite_results in results.items():
            json_results[suite_name] = {
                "suite_info": {
                    "suite_name": suite_results.suite_name,
                    "timestamp": suite_results.timestamp,
                    "total_tests": suite_results.total_tests,
                    "passed_tests": suite_results.passed_tests,
                    "average_quality": suite_results.average_quality,
                    "average_autonomy": suite_results.average_autonomy,
                    "average_response_time": suite_results.average_response_time,
                    "summary": suite_results.summary
                },
                "test_results": [asdict(result) for result in suite_results.test_results]
            }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
            
        logger.info(f"Results saved to: {filepath}")
        return filepath
    
    def generate_report(self, results: Dict[str, TestSuiteResults]) -> str:
        """Generate a comprehensive test report"""
        report = []
        report.append("=" * 80)
        report.append("COMPREHENSIVE MODEL TEST REPORT")
        report.append("=" * 80)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # Overall summary
        total_tests = sum(r.total_tests for r in results.values())
        total_passed = sum(r.passed_tests for r in results.values())
        overall_pass_rate = total_passed / total_tests if total_tests > 0 else 0
        
        report.append("OVERALL SUMMARY")
        report.append("-" * 40)
        report.append(f"Total Tests: {total_tests}")
        report.append(f"Passed: {total_passed}")
        report.append(f"Failed: {total_tests - total_passed}")
        report.append(f"Pass Rate: {overall_pass_rate:.1%}")
        report.append("")
        
        # Per-suite breakdown
        for suite_name, suite_results in results.items():
            report.append(f"TEST SUITE: {suite_name.upper()}")
            report.append("-" * 40)
            report.append(f"Tests: {suite_results.total_tests}")
            report.append(f"Passed: {suite_results.passed_tests}")
            report.append(f"Pass Rate: {suite_results.passed_tests/suite_results.total_tests:.1%}")
            report.append(f"Avg Quality: {suite_results.average_quality:.2f}")
            report.append(f"Avg Autonomy: {suite_results.average_autonomy:.2f}")
            report.append(f"Avg Response Time: {suite_results.average_response_time:.2f}s")
            report.append("")
            
            # Failed tests details
            failed_tests = [r for r in suite_results.test_results if not r.passed]
            if failed_tests:
                report.append(f"Failed Tests in {suite_name}:")
                for test in failed_tests:
                    report.append(f"  โข {test.test_id}: {'; '.join(test.errors)}")
                report.append("")
        
        # Recommendations
        report.append("RECOMMENDATIONS")
        report.append("-" * 40)
        
        if overall_pass_rate < 0.7:
            report.append("โข Overall performance needs improvement")
        if any(r.average_quality < 0.6 for r in results.values()):
            report.append("โข Focus on improving response quality")
        if any(r.average_autonomy < 0.7 for r in results.values()):
            report.append("โข Reduce clarifying questions for better autonomy")
        if any(r.average_response_time > 30 for r in results.values()):
            report.append("โข Optimize for faster response times")
            
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)

def create_mock_model_function():
    """Create a mock model function for testing"""
    def mock_model(prompt: str) -> str:
        # This is a placeholder - replace with your actual model invocation
        time.sleep(0.5)  # Simulate processing time
        return f"This is a mock response to: {prompt[:50]}..."
    return mock_model

def main():
    """Main function to run the comprehensive test"""
    parser = argparse.ArgumentParser(description="Comprehensive Model Testing Framework")
    parser.add_argument("--model-path", help="Path to the model to test")
    parser.add_argument("--test-level", choices=["basic", "intermediate", "advanced", "expert", "autonomous", "all"],
                       default="all", help="Test level to run")
    parser.add_argument("--output", help="Output filename for results")
    
    args = parser.parse_args()
    
    # Initialize tester
    tester = ComprehensiveModelTester(args.model_path)
    
    # Create model function (you'll need to implement this for your specific model)
    # This is where you'd integrate with your model's API
    model_function = create_mock_model_function()
    
    logger.info("Starting comprehensive model test")
    logger.info(f"Test level: {args.test_level}")
    
    # Determine test levels to run
    if args.test_level == "all":
        test_levels = ["basic", "intermediate", "advanced", "autonomous"]
    else:
        test_levels = [args.test_level]
    
    # Run tests
    results = tester.run_all_tests(model_function, test_levels)
    
    # Save results
    results_file = tester.save_results(results, args.output)
    
    # Generate and display report
    report = tester.generate_report(results)
    print("\n" + report)
    
    # Save report
    report_file = tester.results_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"Report saved to: {report_file}")
    
    return results

if __name__ == "__main__":
    main()
