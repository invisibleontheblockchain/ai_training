#!/bin/bash

# Comprehensive Continuous Learning Pipeline Test
set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_header() {
    echo -e "${BLUE}$1${NC}"
}

# Test results tracking
TESTS_PASSED=0
TESTS_FAILED=0

test_result() {
    if [ $1 -eq 0 ]; then
        print_status "‚úÖ $2"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        print_error "‚ùå $2"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

# API base URL
API_URL="http://localhost:8080"

print_header "üß™ Ghosteam V5 Continuous Learning Pipeline Test"

# Activate virtual environment if it exists
if [ -d "venv" ]; then
    source venv/bin/activate
fi

# Test 1: API Health Check
print_header "Test 1: API Health Check"
response=$(curl -s -o /dev/null -w "%{http_code}" "$API_URL/health")
test_result $([[ "$response" == "200" ]] && echo 0 || echo 1) "API health check"

if [ "$response" != "200" ]; then
    print_error "API is not responding. Please ensure services are running."
    exit 1
fi

# Test 2: Feature Store Connectivity
print_header "Test 2: Feature Store Connectivity"
python3 << 'EOF'
import sys
try:
    from src.data.feature_store.feast_config import FeatureStoreManager
    fs = FeatureStoreManager()
    feature_views = fs.list_feature_views()
    print(f"‚úÖ Feature store connected. Found {len(feature_views)} feature views")
    sys.exit(0)
except Exception as e:
    print(f"‚ùå Feature store connection failed: {e}")
    sys.exit(1)
EOF
test_result $? "Feature store connectivity"

# Test 3: Train Initial Model
print_header "Test 3: Train Initial Model"
python3 << 'EOF'
import sys
import pandas as pd
import numpy as np
from src.models.training.sklearn_models import SklearnClassifier
import mlflow

try:
    # Load training data
    training_data = pd.read_parquet('data/processed/training_data.parquet')
    
    # Prepare features and target
    feature_cols = ['age', 'activity_score', 'engagement_rate', 'total_sessions', 
                   'avg_session_duration', 'days_since_signup']
    X = training_data[feature_cols]
    y = training_data['target']
    
    # Train model
    model = SklearnClassifier(
        model_name="ghosteam_v5_test_model",
        algorithm="random_forest",
        n_estimators=50,
        random_state=42
    )
    
    results = model.train(X, y, validation_split=0.2)
    
    # Log to MLflow
    model.log_to_mlflow("ghosteam_v5_testing")
    
    # Save model locally
    model.save_model("data/models/initial_model")
    
    print(f"‚úÖ Model trained successfully. Accuracy: {results['metrics']['train_accuracy']:.3f}")
    sys.exit(0)
    
except Exception as e:
    print(f"‚ùå Model training failed: {e}")
    sys.exit(1)
EOF
test_result $? "Initial model training"

# Test 4: Model Serving
print_header "Test 4: Model Serving"

# First, we need to register the model in MLflow for serving
python3 << 'EOF'
import mlflow
from mlflow.tracking import MlflowClient

try:
    client = MlflowClient()
    
    # Get the latest run
    experiment = mlflow.get_experiment_by_name("ghosteam_v5_testing")
    if experiment:
        runs = client.search_runs(experiment.experiment_id, order_by=["start_time DESC"], max_results=1)
        if runs:
            run_id = runs[0].info.run_id
            model_uri = f"runs:/{run_id}/model"
            
            # Register model
            model_version = mlflow.register_model(model_uri, "ghosteam_v5_test_model")
            
            # Transition to production
            client.transition_model_version_stage(
                name="ghosteam_v5_test_model",
                version=model_version.version,
                stage="Production"
            )
            
            print(f"‚úÖ Model registered and promoted to production: version {model_version.version}")
        else:
            print("‚ùå No runs found")
    else:
        print("‚ùå Experiment not found")
        
except Exception as e:
    print(f"‚ùå Model registration failed: {e}")
EOF

# Test prediction endpoint
sleep 5  # Give time for model to be available

prediction_response=$(curl -s -X POST "$API_URL/predict" \
    -H "Content-Type: application/json" \
    -d '{
        "entity_ids": ["user_000001", "user_000002"],
        "features": ["user_features:age", "user_features:activity_score"],
        "model_name": "ghosteam_v5_test_model"
    }')

if echo "$prediction_response" | grep -q "predictions"; then
    test_result 0 "Model serving and prediction"
    print_status "Prediction response: $(echo $prediction_response | jq -r '.predictions')"
else
    test_result 1 "Model serving and prediction"
    print_warning "Response: $prediction_response"
fi

# Test 5: Drift Detection
print_header "Test 5: Drift Detection"
python3 << 'EOF'
import sys
import pandas as pd
from src.monitoring.drift_detection.drift_detector import DriftDetector

try:
    # Load reference and drift data
    reference_data = pd.read_parquet('data/processed/reference_data.parquet')
    drift_data = pd.read_parquet('data/processed/drift_test_mean.parquet')
    
    # Initialize drift detector
    detector = DriftDetector(methods=["ks", "mmd"], threshold=0.05)
    detector.fit(reference_data)
    
    # Detect drift
    drift_results = detector.detect_drift(drift_data)
    
    print(f"Drift detected: {drift_results['drift_detected']}")
    print(f"Drift score: {drift_results['overall_drift_score']:.3f}")
    print(f"Drifted features: {drift_results['summary']['drifted_features']}")
    
    if drift_results['drift_detected']:
        print("‚úÖ Drift detection working correctly")
        sys.exit(0)
    else:
        print("‚ö†Ô∏è  No drift detected (may be expected)")
        sys.exit(0)
        
except Exception as e:
    print(f"‚ùå Drift detection failed: {e}")
    sys.exit(1)
EOF
test_result $? "Drift detection system"

# Test 6: Automated Retraining Trigger
print_header "Test 6: Automated Retraining Simulation"
python3 << 'EOF'
import sys
import pandas as pd
import numpy as np
from src.models.training.sklearn_models import SklearnClassifier
from src.monitoring.drift_detection.drift_detector import DriftDetector
import mlflow

try:
    print("Simulating automated retraining workflow...")
    
    # 1. Load current data with drift
    current_data = pd.read_parquet('data/processed/drift_test_mean.parquet')
    reference_data = pd.read_parquet('data/processed/reference_data.parquet')
    
    # 2. Detect drift
    detector = DriftDetector(methods=["ks", "mmd"], threshold=0.05)
    detector.fit(reference_data)
    drift_results = detector.detect_drift(current_data)
    
    print(f"Drift detected: {drift_results['drift_detected']}")
    
    # 3. If drift detected, trigger retraining
    if drift_results['drift_detected'] or True:  # Force retraining for demo
        print("üîÑ Triggering automated retraining...")
        
        # Generate new training data (simulating fresh data collection)
        training_data = pd.read_parquet('data/processed/training_data.parquet')
        
        # Add some of the drift data to training set
        feature_cols = ['age', 'activity_score', 'engagement_rate', 'total_sessions', 
                       'avg_session_duration', 'days_since_signup']
        
        # Create target for drift data (simulate labeling)
        drift_subset = current_data[feature_cols].sample(500)
        drift_targets = np.random.binomial(1, 0.4, len(drift_subset))  # Slightly different distribution
        
        drift_training = drift_subset.copy()
        drift_training['target'] = drift_targets
        
        # Combine with original training data
        combined_training = pd.concat([training_data, drift_training], ignore_index=True)
        
        X = combined_training[feature_cols]
        y = combined_training['target']
        
        # Train new model
        retrained_model = SklearnClassifier(
            model_name="ghosteam_v5_test_model_retrained",
            algorithm="random_forest",
            n_estimators=50,
            random_state=42
        )
        
        results = retrained_model.train(X, y, validation_split=0.2)
        
        # Log to MLflow
        retrained_model.log_to_mlflow("ghosteam_v5_retraining")
        
        print(f"‚úÖ Retraining completed. New accuracy: {results['metrics']['train_accuracy']:.3f}")
        
        # 4. Model validation (simplified)
        if results['metrics']['train_accuracy'] > 0.6:  # Simple threshold
            print("‚úÖ Model validation passed")
            
            # 5. Register new model version
            client = mlflow.tracking.MlflowClient()
            experiment = mlflow.get_experiment_by_name("ghosteam_v5_retraining")
            if experiment:
                runs = client.search_runs(experiment.experiment_id, order_by=["start_time DESC"], max_results=1)
                if runs:
                    run_id = runs[0].info.run_id
                    model_uri = f"runs:/{run_id}/model"
                    
                    # Register new version
                    model_version = mlflow.register_model(model_uri, "ghosteam_v5_test_model")
                    
                    print(f"‚úÖ New model version registered: {model_version.version}")
                    
                    # In production, you would gradually transition traffic
                    print("üöÄ Ready for gradual deployment")
        else:
            print("‚ùå Model validation failed")
            
    print("‚úÖ Automated retraining workflow completed")
    sys.exit(0)
    
except Exception as e:
    print(f"‚ùå Automated retraining failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
EOF
test_result $? "Automated retraining workflow"

# Test 7: Monitoring Metrics
print_header "Test 7: Monitoring Metrics"

# Check if metrics are being collected
metrics_response=$(curl -s "$API_URL/metrics")
if echo "$metrics_response" | grep -q "total_predictions\|average_latency_ms"; then
    test_result 0 "Monitoring metrics collection"
    print_status "Metrics: $(echo $metrics_response | jq -r '.total_predictions // 0') predictions, $(echo $metrics_response | jq -r '.average_latency_ms // 0')ms avg latency"
else
    test_result 1 "Monitoring metrics collection"
fi

# Test 8: End-to-End Workflow
print_header "Test 8: End-to-End Workflow Validation"
python3 << 'EOF'
import sys
import pandas as pd
import requests
import json
from src.data.feature_store.feast_config import FeatureStoreManager

try:
    print("Testing complete end-to-end workflow...")
    
    # 1. Feature Store Integration
    fs = FeatureStoreManager()
    
    # 2. Get online features
    entity_rows = [{"user_id": "user_000001"}, {"user_id": "user_000002"}]
    features = ["user_features:age", "user_features:activity_score"]
    
    feature_vector = fs.get_online_features(features, entity_rows)
    feature_df = feature_vector.to_df()
    
    print(f"‚úÖ Retrieved features for {len(feature_df)} entities")
    
    # 3. Make prediction via API
    response = requests.post("http://localhost:8080/predict", json={
        "entity_ids": ["user_000001", "user_000002"],
        "features": features,
        "model_name": "ghosteam_v5_test_model"
    })
    
    if response.status_code == 200:
        result = response.json()
        print(f"‚úÖ Prediction successful. Latency: {result['latency_ms']:.1f}ms")
        print(f"   Predictions: {result['predictions']}")
    else:
        print(f"‚ùå Prediction failed: {response.text}")
        sys.exit(1)
    
    # 4. Batch prediction test
    batch_response = requests.post("http://localhost:8080/predict/batch", json={
        "data": [
            {"age": 25, "activity_score": 0.8},
            {"age": 35, "activity_score": 0.6}
        ],
        "model_name": "ghosteam_v5_test_model"
    })
    
    if batch_response.status_code == 200:
        batch_result = batch_response.json()
        print(f"‚úÖ Batch prediction successful. Batch size: {batch_result['batch_size']}")
    else:
        print(f"‚ùå Batch prediction failed: {batch_response.text}")
    
    print("‚úÖ End-to-end workflow validation completed")
    sys.exit(0)
    
except Exception as e:
    print(f"‚ùå End-to-end workflow failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
EOF
test_result $? "End-to-end workflow validation"

# Test Summary
print_header "üèÅ Test Summary"
echo ""
echo "Tests Passed: $TESTS_PASSED"
echo "Tests Failed: $TESTS_FAILED"
echo "Total Tests: $((TESTS_PASSED + TESTS_FAILED))"
echo ""

if [ $TESTS_FAILED -eq 0 ]; then
    print_status "üéâ All tests passed! Continuous learning pipeline is working correctly."
    echo ""
    echo "‚úÖ System Validation Complete:"
    echo "   ‚Ä¢ API serving is functional"
    echo "   ‚Ä¢ Feature store is operational"
    echo "   ‚Ä¢ Model training and registration works"
    echo "   ‚Ä¢ Drift detection is active"
    echo "   ‚Ä¢ Automated retraining pipeline functions"
    echo "   ‚Ä¢ Monitoring and metrics collection works"
    echo "   ‚Ä¢ End-to-end workflow is validated"
    echo ""
    echo "üöÄ Ready for production deployment!"
else
    print_warning "‚ö†Ô∏è  Some tests failed. Please review the output above."
    echo ""
    echo "Common issues:"
    echo "‚Ä¢ Services not fully started (wait a few minutes and retry)"
    echo "‚Ä¢ Feature store not properly initialized"
    echo "‚Ä¢ MLflow tracking server connectivity"
    echo ""
    echo "Check logs with: docker-compose logs [service_name]"
fi

echo ""
echo "Next steps:"
echo "1. Monitor dashboards: http://localhost:3000"
echo "2. Check MLflow experiments: http://localhost:5000"
echo "3. View API documentation: http://localhost:8080/docs"
echo "4. Deploy to production: ./scripts/deploy.sh"
